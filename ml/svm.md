## 支持向量机(SVM)

SVM 是一种二类分类模型。它的基本思想是在特征空间中寻找间隔最大的分离超平面使数据得到高效的二分类.支持向量机学习方法有一些由简至繁的模型:
- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

### 定义
数据集：$\{X_i,y_i|i=1,2,\cdots\}$,$X_i\in R^d$,$y_i \in \{-1,1\}$

### 推导
从最简单的线性可分的情况下开始，在该前提下，可以找出如下超平面切分正例和反例：
$$
    W^TX+b=0
$$
并且有如下条件：
$$
\left\{\begin{matrix}
    W^TX_i+b >=1 & y_i = 1 \\ 
    W^TX_i+b <=-1 & y_i = -1 
\end{matrix}\right.
$$
即如下图所示：
![](images/2021-07-28-23-42-54.png)
其中,$\frac{2}{||W||}$是$W^TX_i+b ==1$和$W^TX_i+b ==-1$之间的垂直距离。
SVM的优化目标就是找出这两个超平面，使得他们之间的垂直距离最大。该优化问题可以表述为：
$$
\begin{split}
    \max\limits_{W,B}\frac{2}{||W||} \\
    s.t. y_i(W^TX_i+b) >= 1
\end{split}
$$
该问题等价于：
$$
\begin{split}
    \min\limits_{W,B}\frac{||W||}{2} \\
    s.t. y_i(W^TX_i+b) >= 1
\end{split}
$$