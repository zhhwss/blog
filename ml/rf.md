<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
    	MathJax.Hub.Config({tex2jax: {
             inlineMath: [['$','$']],
             displayMath: [["\\(","\\)"],["\\[","\\]"]],
             processEscapes: true
           }
         });
    </script>
</head>


## Random Forest
随机森林（Random Forest，简称RF）是集成学习(ensemble learning)的一种。其采用Bagging策略来训练CART分类回归树，然后将单树的结果综合一下得到最终的预测结果。其中，其有如下几个特点：
- 采用Bagging策略来训练CART分类回归树，因此各个子树之间是相互独立的，可以分别并行训练
- 其次每棵子树选择分裂特征时，采用随机策略跳出$k=log_2 d$个特征(d是总特征数)，并在其上再挑选最优的特征和切分点。这样增加每棵子树的随机性，使得整体的泛化能力更强。

Bagging时有有放回的重复采样，假设一个数据有n个样本，那么一个样本没有被选中的概率是
$$
    p = (1-\frac{1}{n})^n
$$
当n足够大的时候:
$$
    \lim_{n\rightarrow \infty} p = (1-\frac{1}{n})^n= \frac{1}{e} \approx 36.8\%
$$
因此，在baggging采样的一个数据集上，样本没选中的概率是36.8\%，因此有36.8\%的数据不在某个子树的训练中使用。**这部分数据也被称为包外数据。**



### 影响RF的分类性能的主要因素：
- RF中单棵决策树的分类强度：每棵树的分类强度越大，则组合成的随机森林的分类性能就更强
- RF中任意两棵树之间的相关度：树之间的相关度越大，那么随机森林的分类性能就越差

### 优缺点
#### 优点
- 可以直接处理高维的数据，不需要进行feature的extraction
- 非常容易进行分布式的处理，实现起来非常高效
- 可以使用袋外估计(OOB)数据评价算法的误差率，而不用像一般的算法那样，还需要设置一个validation set去衡量误差
- 因为最终是通过投票和平均的方式做prediction，相对光滑，受噪声干扰小，鲁棒性强
- CART的优点：可以处理非线性问题，可以非常好的处理数值型和序列型的变量
#### 缺点
- 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟合。
- 比决策树算法更复杂，计算成本更高。
- 由于其本身的复杂性，它们比其他类似的算法需要更多的时间来训练。

### 基于RF的特征重要性评估方法
#### 基于Gini系数
首先Gini系数的定义如下：
$$
  Gini(D) = 1- \sum_{k=1}^K (\frac{|C_k|}{|D|})^2 = 1-\sum_{k=1} p_k^2
$$
$p_k$是第k类占整体样本的比例。
特征$X_j$在节点m上的重要性，可以用节点分支前后Gini系数变化量来衡量:
$$
    VIM_{jm} = GI_m - GI_l - GI_r
$$
则特征$X_j$的
重要性就是在所有子树的所有节点上特征X_j为分裂特征的Gini系数变化的和：
$$
    VIM_{j} = \sum_{T_i \in RF}\sum_{m \in T_i} VIM_{jm}
$$
最后做一个归一化处理，即可得到特征$X_j$的重要性：
$$
    VIM_{j} = \frac{VIM_{j}}{\sum_{i=1}^d VIM_i}
$$

#### 利用袋外数据（OOB）错误率计算
- 对于一棵树$T_i$，用OOB样本可以得到误差$e_1$，然后随机改变OOB中的特征$X_j$，保持其他特征不变，对特征$X_j$进行随机的上下置换，得到误差$e_2$。至此，可以用$e_1 - e_2$来刻画特征$X_j$的重要性。其依据就是，如果一个特征很重要，那么其变动后会非常影响测试误差，如果测试误差没有怎么改变，则说明特征$X_j$不重要。
- 对于有M颗树的随机森林，$X_j$的重要性为$\frac{1}{M}\sum_{i=1}^M e^{(i)}_1 - e^{(i)}_2$