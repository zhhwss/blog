## 强化学习

### 强化学习适用场景
强化学习适用于多步决策问题。要理解强化学习问题，首先要区分它与单步决策问题。单步决策问题在做决策的时候，仅考虑当前状态，不考虑未来的情况，因此它是**贪心选择**当前状态下收益最大的决策行动。而多步决策问题需要考虑未来状态的影响，它在做决策的时候目标是**使得从现在到终止**收益最大化。简而言之，可以将强化学习问题同单步决策问题之间的关系类比于**动态规划**和**贪心算法**。因此适用于强化学习的场景有如下特征：
- 当前状态决策会影响下一步状态，进而影响它下一步的收益
- 当前的目标是一列决策后的整体收益最大化

注意，**过于历史对当前决策的影响不是强化学习适用的特征。** 因此，如果不具备上述特征，把问题规约成单步优化问题可能更有效。

### 逆强化学习适用场景
逆强化学习的出发点是不知道Reward函数，但是知道一系列专家轨迹，目标是从专家轨迹中学出Reward函数，进而学出强化学习的决策模型。
因此，它同使用于强化学习的场景类似：
- 当前状态决策会影响下一步状态，进而影响它下一步的收益
- 当前的目标是一列决策后的整体收益最大化
- 专家路径决策是按照上述两条规则，**即每步决策的时候会考虑未来的状态的收益**
- 专家轨迹是收敛的：
  - 环境是近似稳定的，不会因为环境变化导致专家决策发生变化
  - 专家的决策策略是收敛的，换言之就是Reward函数是收敛的

为什么会多出第三条呢？因为逆强化学习是去学习专家轨迹中的Reward函数。就算这是一个强化学习场景，如果专家轨迹采取贪心策略，那也不太适合用逆强化学习来解决（笔者认为不是逆强化学习解决不了，而是解决起来费劲）。举例说明，在新闻推送场景下，用户会根据推送内容点击新闻，推荐系统接受到用户的反馈后会根据反馈进一步推动新内容给用户，如此反复迭代。**在该场景下，我们来的用户决策。**
- 我们将推送过来的信息中有多个篇用户喜欢作为当前的决策的Reward。
- 用户当前的决策会影响用户未来的接受到的新闻，因此会影响用户未来的收益。
- 然而，大部分用户，只是根据当前推送给他的新闻，从中挑选出自己的喜欢的，并不会考虑未来会推送多少篇自己喜欢的给自己。
- 少部分用户可能会考虑，让系统多推送或者少推送某些内容，进而影响用户当前的决策。但是，经过一段时间磨合之后，又会陷入只挑选出自己喜欢的。并且，上述策略一般出现在用户想多了解某一方面内容的时候，更像是用户兴趣变化的过程。并且，系统并不稳定，用户的兴趣爱好有会发生变化，因此，用户的决策路径大概率不是收敛的。

第四条怎么理解呢？因为逆强化学习Reward函数未知，因此Reward函数可能不是收敛的。对于同一状态下同一决策，如果reward或者到下一个状态的转移概率会发生剧烈抖动，也是无法学好的。而这种抖动产生的原因可能是环境发生变化，也可能是reward函数发生变化。还是上面的例子：
- 如果推荐系统发生变化就是环境发生变化。
- 如果用户兴趣发生变化，就是Reward函数发生变化。

总是，该场景下用户大概率不会根据未来的推送的内容对当前新闻进行的点击决策。少部分用户短期可能有改变自己接受内容的意愿，但那更多是用户兴趣发生变化，用户最终还是指挥挑选出自己喜欢的，并且这种改变不是收敛的。