## Residual network

### 主要要解决的问题
#### 模型退化

模型退化与过拟合不同
![](images/2021-08-02-23-09-01.png)

在这个多项式回归问题中，左边的模型是欠拟合（under fit）的此时有很高的偏差（high bias），中间的拟合比较成功，而右边则是典型的过拟合（overfit），此时由于**模型过于复杂**，导致了高方差（high variance）。

然而，很明显当前CNN面临的效果退化不是因为过拟合，因为过拟合的现象是"高方差，低偏差"，即测试误差大而训练误差小。但实际上，深层CNN的训练误差和测试误差都很大。
![](images/2021-08-02-23-10-19.png)

##### 模型退化产生原因

理说，当我们堆叠一个模型时，理所当然的会认为效果会越堆越好。因为，假设一个比较浅的网络已经可以达到不错的效果，那么即使之后堆上去的网络什么也不做，模型的效果也不会变差。然而事实上，这却是问题所在。“什么都不做”恰好是当前神经网络最难做到的东西之一。MobileNet V2的论文[2]也提到过类似的现象，由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的（信息损失）。我们很难从输出反推回完整的输入。

![](images/2021-08-02-23-11-50.png)

因此，可以认为Residual Network的初衷，其实是让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化！

#### 梯度爆炸/消失

除此之外，最受人认可的原因就是“梯度爆炸/消失（弥散）”了。

有一些理论认为[Bacth Normalization](./bn.md)(简称BN)可以解决“梯度爆炸/消失(弥散)”的问题，而BN的作用本质上是rescale每层输入到一个网络认为合理的范围，避免了由于输入分布集中在饱和区带来的梯度弥散现象。然而，笔者认为BN并不能完全消除“梯度爆炸/消失(弥散)”的问题。原因包括如下：
- 通常神经网络的激活函数为ReLu，不存在饱和区带来的“梯度弥散”的问题
- “梯度爆炸/消失(弥散)”主要是因为反向传播时，链式求导乘积导致的，这一点BN无法消除

因此，Residual Network要解决的另一问题就是梯度爆炸/消失。

### skip connect
Residual Network就是如图所示的skip connect, 即构造一个block，是其输出等于：
$$
    F(x)+x
$$
![](images/2021-08-03-16-02-16.png)
其中，将输入x直接连接到输出上的结构称之为skip connect。
多个skip block串联到一起就构成了Residual Network如下图所示：
![](images/2021-08-03-16-08-48.png)

上述结构为什么有效呢？我们可以从以下几个方面来分析：
- 每一层的输出都与最终的输出直接相连，因此梯度反向传导到每一层的路径更短，减缓了链式求导乘积导致的梯度爆炸/消失。这可以提高训练效率。
- **如上图所示，Residual Network可看作为不同层数的神经网络的ensemble，其中，这些神经网络共享参数。** Residual Network可以从中自动挑选出最适合当前任务的层数，取得好的结果。这样就可以减缓模型退化带来的问题。