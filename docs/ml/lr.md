<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
    	MathJax.Hub.Config({tex2jax: {
             inlineMath: [['$','$']],
             displayMath: [["\\(","\\)"],["\\[","\\]"]],
             processEscapes: true
           }
         });
    </script>
</head>

## 逻辑回归

### 定义
逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。

### 逻辑回归的基本假设

- 逻辑回归假设数据服从[伯努利分布](../statics/bernoulli_distribution.md)，伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是𝑝,抛中为负面的概率是1−𝑝.在逻辑回归这个模型里面是假设$h_\theta(x)$ 为样本为正的概率，1−$h_\theta(x)$为样本为负的概率。那么整个模型可以描述为:
$$
    \begin{split}
    P(Y=1|x)&=p=h_\theta(x) \\
    P(Y=0|x)&=1-p=1-h_\theta(x) \\
    \end{split}
$$
即
$$
    P(Y|x) =p=h_\theta(x)^Y(1-h_\theta(x))^{1-Y} 
$$



- 考虑二分类问题，给定数据集
$$
    D=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in R^n,y\in \{0,1\}
$$
在给定参数$\theta$下的概率为:
$$
    P(D|\theta)=\prod_{i=1}^N h_\theta(x_i)^{y_i}(1-h_\theta(x_i))^{1-y_i}
$$

-  逻辑回归中采用sigmoid的函数表征$x\rightarrow p$的映射,即
$$
    p=h_\theta(x)=\frac{1}{1+e^{-\theta^T x}}
$$
其函数图如下图所示:<br/>
![](images/2021-07-25-22-55-03.png)

### 最大似然估计
**极大似然估计就是求解参数$\theta$使得概率$P(D|\theta)$最大**。通常，我们可以对似然函数取log变换为
$$
     L(\theta)=log(P(D|\theta))=\sum_{i=1}^N y_i log h_\theta(x_i) + (1-y_i)log(1-h_\theta(x_i))
$$
在机器学习中我们有损失函数的概念，其衡量的是模型预测错误的程度。如果取整个数据集上的平均对数似然损失，我们可以得到:
$$
    J(\theta)=-\frac{1}{N}\sum_{i=1}^N y_i log h_\theta(x_i) + (1-y_i)log(1-h_\theta(x_i))\tag{1}
$$
在逻辑回归模型中，我们最大化似然函数和最小化损失函数实际上是等价的。上式也被成为交叉熵(Cross Entropy)损失函数。

### 求解
求解逻辑回归，优化目标是求得式(1)的极小值对应的$\theta$,可采用梯度下降法来进行求解。
$J(\theta)$关于$\theta$的梯度计算公式为
$$
\frac{\partial J}{\partial \theta} = -\frac{1}{N}\sum_{i=1}^N (y_i \frac{1}{h_\theta(x_i)} + (1-y_i)\frac{1}{1-h_\theta(x_i)})\frac{\partial h_\theta(x_i)}{\partial \theta}
$$
对于sigmoid函数$h_\theta(x_i)$,其关于$x_i$的倒数为：
$$
\frac{\partial h_\theta(x_i)}{\partial \theta}=h_\theta(x_i)(1-h_\theta(x_i))x_i
$$
带入上式可得
$$
\frac{\partial J}{\partial \theta}=-\frac{1}{N}\sum_{i=1}^N (y_i-h_\theta(x_i))x_i
$$
梯度下降更新公式为:
$$
    \theta = \theta - \alpha \frac{\partial J}{\partial \theta}=\theta + \alpha \frac{1}{N}\sum_{i=1}^N (y_i-h_\theta(x_i))x_i
$$
这里$\alpha$是学习率。梯度方式是函数$J(\theta)$增大最快的方向，反向则为减小最快的方向。

梯度下降法通常有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，其优缺点如下：
- 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
- 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
- 小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。

### 优缺点
优点:
- 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
- 模型效果不错。在工程上是可以接受的（作为baseline)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
- 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
- 资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
- 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)。

缺点:
- 准确率并不是很高。因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布。
- 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
- 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。

### 知识点
1. 为什么逻辑回归比线性回归要好?<br/>
逻辑回归和线性回归首先都是广义的线性回归。经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

2. 逻辑回归与最大熵模型MaxEnt的关系?<br/>
逻辑回归跟最大熵模型没有本质区别。逻辑回归是最大熵对应类别为二类时的特殊情况，也就是当逻辑回归类别扩展到多类别时，就是最大熵模型。

3. 逻辑回归的求解方法？<br/>
梯度下降法，随机梯度下降法，牛顿法，LBFGS，BFGS,OWLQN

4. 工程上，怎么实现LR的并行化？有哪些并行化的工具?<br/>
LR的并行化分为两个层次：
- 对Batch内不同sample计算梯度的并行化
- 拟用矩阵并行计算在单个sample上计算梯度的并行化

5. 为什么逻辑回归要使用极大似然函数呢?<br/>
- 似然函数表征了数据整体的预测概率，极大似然函数就是让对数据样本整体的预测概率最大。
- 对极大似然函数取对数以后相当于对数损失函数，由上面 梯度更新 的公式可以看出，对数损失函数的训练求解参数的速度是比较快的，而且更新速度只和x，y有关，比较的稳定。
- 为什么不用平方损失函数呢，如果使用平方损失函数，梯度更新的速度会和 sigmod 函数的梯度相关，sigmod 函数在定义域内的梯度都不大于0.25，导致训练速度会非常慢。